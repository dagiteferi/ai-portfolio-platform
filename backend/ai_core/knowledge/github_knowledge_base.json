[
    {
        "title": "ai-portfolio-platform",
        "url": "https://github.com/dagiteferi/ai-portfolio-platform",
        "content": "None\n\nREADME:\n# Update-Persnoal-website\nupdate the front end and add some back end and adding chatbot"
    },
    {
        "title": "Attendance-System",
        "url": "https://github.com/dagiteferi/Attendance-System",
        "content": "QR base Attendance system"
    },
    {
        "title": "AVL-Tree-Implementation",
        "url": "https://github.com/dagiteferi/AVL-Tree-Implementation",
        "content": "AVL Tree Implementation  --------------------- 1.Insert Element into the tree 2.Delete Element into the tree 3.Display Balanced AVL Tree 4.InOrder traversal 5.PreOrder traversal 6.PostOrder traversal 7.Exit Enter your Choice:\n\nREADME:\n# AVL-Tree-Implementation\nAVL Tree Implementation  --------------------- 1.Insert Element into the tree 2.Delete Element into the tree 3.Display Balanced AVL Tree 4.InOrder traversal 5.PreOrder traversal 6.PostOrder traversal 7.Exit Enter your Choice:"
    },
    {
        "title": "BinarySearchTree",
        "url": "https://github.com/dagiteferi/BinarySearchTree",
        "content": "display BST"
    },
    {
        "title": "brent-price-change-analysis",
        "url": "https://github.com/dagiteferi/brent-price-change-analysis",
        "content": "The aim of this project is to analyze Brent oil price fluctuations by detecting change points and identifying their causes using statistical modeling. This includes Bayesian methods, ARIMA, GARCH, and interactive visualizations to provide insights for investors, policymakers, and analysts\n\nREADME:\n# Brent Price Change Analysis and Statistical Modeling of Brent Oil Prices 🌍📈\n\n## Table of Contents 📚\n\n- [Project Overview](#project-overview)\n- [Key Objectives](#key-objectives)\n- [Data](#data)\n- [Methodology](#methodology)\n  - [Data Analysis Workflow](#1️⃣-data-analysis-workflow)\n  - [Statistical & Machine Learning Models](#2️⃣-statistical--machine-learning-models)\n  - [Change Point Detection Techniques](#3️⃣-change-point-detection-techniques)\n- [Key Features of the Dashboard](#key-features-of-the-dashboard)\n- [Tech Stack](#tech-stack)\n- [Project Structure](#project-structure)\n- [Results & Insights](#results--insights)\n- [References & Further Reading](#references--further-reading)\n- [How to Run the Project](#how-to-run-the-project)\n- [Contributing](#contributing)\n\n## Project Overview\n\nThe aim of this project is to analyze Brent oil price fluctuations by detecting change points and identifying their causes using statistical modeling. By leveraging time series models such as ARIMA, GARCH, and Bayesian methods (PyMC3), we investigate how economic, political, and regulatory events impact oil prices. The insights generated will help investors, policymakers, and analysts make informed decisions. 💡\n\n## Key Objectives 🎯\n\n- Detect change points in Brent oil prices over the past decades. 📊\n- Analyze the impact of key events (e.g., political decisions, economic sanctions, OPEC policies) on price fluctuations. 🌐\n- Apply statistical modeling techniques such as Bayesian inference, ARIMA, and GARCH. 📉\n- Develop an interactive dashboard using Flask and React for visualizing results. 🖥️\n- Provide actionable insights for investment strategies, policy development, and risk management. 📈\n\n## Data 📂\n\n- **Source:** Historical Brent oil prices dataset (1987 - 2022).\n- **Fields:**\n  - `Date`: Daily recorded price (Format: DD-MMM-YY).\n  - `Price`: Brent oil price in USD per barrel.\n\n## Methodology 🔍\n\n### 1️⃣ Data Analysis Workflow\n\n- Data cleaning and preprocessing 🧹\n- Exploratory Data Analysis (EDA) 📊\n- Time series modeling and change point detection 📈\n- Model evaluation and selection ✅\n- Interpretation of findings 🔍\n\n### 2️⃣ Statistical & Machine Learning Models\n\n- **Time Series Models:** ARIMA, GARCH 📉\n- **Bayesian Modeling:** Bayesian Change Point Detection (PyMC3) 📊\n- **Machine Learning:** LSTM (Long Short-Term Memory) for deep learning analysis 🤖\n- **Econometric Models:** VAR (Vector Autoregression) for multivariate analysis 📈\n\n### 3️⃣ Change Point Detection Techniques\n\n- Bayesian Change Point Analysis 🔍\n- Likelihood Ratio Tests ⚖️\n- CUSUM (Cumulative Sum Control Chart) 📊\n- Pettitt’s Test 🧪\n- Segmented Regression 📈\n\n## Key Features of the Dashboard 🖥️\n\n- Interactive visualizations of Brent oil price trends and change points. 📊\n- Event highlighting: See price shifts corresponding to major economic/political events. 🌍\n- Custom filters: Explore price movements across different timeframes. ⏳\n- Model performance metrics: Evaluate prediction accuracy. 📏\n\n## Tech Stack ⚙️\n\n- **Programming Languages:** Python, JavaScript 🐍💻\n- **Backend:** Flask (API for serving model results) 🔌\n- **Frontend:** React (for interactive visualization) 📱\n- **Libraries & Tools:** Pandas, NumPy, Matplotlib, Seaborn, PyMC3, Statsmodels, Scikit-learn, D3.js, Recharts 📚\n\n## Project Structure 📁\n\n\n  ```bash\n     Directory structure:\n└── dagiteferi-brent-price-change-analysis/\n    ├── README.md\n    ├── file_structure.py\n    ├── requirements.txt\n    ├── docs/\n    │   └── data_analysis_workflow.md\n    ├── logs/\n    ├── models/\n    │   ├── X_scaler.pkl\n    │   ├── lstm_model.h5\n    │   └── y_scaler.pkl\n    ├── notebooks/\n    │   ├── README.md\n    │   ├── __init__.py\n    │   ├── changepointanalysis.ipynb\n    │   ├── eda.ipynb\n    │   └── logs/\n    ├── oil-price-dashboard/\n    │   ├── backend/\n    │   │   ├── app.py\n    │   │   └── evaluation_results.pkl\n    │   └── frontend/\n    │       ├── README.md\n    │       ├── README.old.md\n    │       ├── package-lock.json\n    │       ├── package.json\n    │       ├── .gitignore\n    │       ├── public/\n    │       │   ├── index.html\n    │       │   ├── manifest.json\n    │       │   └── robots.txt\n    │       └── src/\n    │           ├── App.css\n    │           ├── App.js\n    │           ├── App.test.js\n    │           ├── index.css\n    │           ├── index.js\n    │           ├── reportWebVitals.js\n    │           └── setupTests.js\n    ├── scripts/\n    │   ├── README.md\n    │   ├── AdaptingModel.py\n    │   ├── __init__.py\n    │   ├── analyzer.py\n    │   ├── eda.py\n    │   ├── logger.py\n    │   ├── oil_price_analysis.py\n    │   └── visualizer.py\n    ├── src/\n    │   ├── __init__.py\n    │   ├── data_loading.py\n    │   └── fetcher.py\n    ├── tests/\n    │   └── __init__.py\n    └── .github/\n        └── workflows/\n            └── unittests.yml\n  ```\n\n\n## How to Run the Project 🚀\n\n1. **Clone the Repository**\n   ```bash\n   git clone https://github.com/dagiteferi/brent-price-change-analysi.git\n   cd brent-price-change-analysi\n    ```\n2. **Install Dependencies**\n```bash\n    pip install -r requirements.txt\n```\n3. **Run the Flask Backend**\n```bash\n   cd dashboard/backend\n  flask run\n\n```\nThe backend should now be running on http://127.0.0.1:5000/.\n4. **Start the React Frontend**\n   ```bash\n cd dashboard/frontend\nnpm start\n\n   ```\nThe frontend should now be running on http://localhost:3000/\n\n\n**Contributing 🤝**\n\nContributions are welcome! Feel free to open issues and pull requests. Specific areas where contributions are especially welcome include model development, dashboard features, and documentation."
    },
    {
        "title": "Calculator",
        "url": "https://github.com/dagiteferi/Calculator",
        "content": "None\n\nREADME:\n# calculator\n\nA new Flutter project.\n\n## Getting Started\n\nThis project is a starting point for a Flutter application.\n\nA few resources to get you started if this is your first Flutter project:\n\n- [Lab: Write your first Flutter app](https://docs.flutter.dev/get-started/codelab)\n- [Cookbook: Useful Flutter samples](https://docs.flutter.dev/cookbook)\n\nFor help getting started with Flutter development, view the\n[online documentation](https://docs.flutter.dev/), which offers tutorials,\nsamples, guidance on mobile development, and a full API reference."
    },
    {
        "title": "dagiteferi",
        "url": "https://github.com/dagiteferi/dagiteferi",
        "content": "Config files for my GitHub profile.\n\nREADME:\n<h1 align=\"center\">Hi 👋, I'm <span style=\"color:#2e86de;\">Dagmawi Teferi</span></h1>\n<h3 align=\"center\" style=\"font-style: italic; color: #555;\">A Full-Stack Developer & AI/ML Engineer</h3>\n\n<p align=\"censdter\" style=\"max-width: 800px; margin: auto; font-size: 17px; line-height: 1.6; color: #333;\">\n  My journey into tech began with a spark of curiosity when my ICT teacher first introduced me to how software shapes the digital world. \n  That spark quickly turned into a passion. In high school, I began crafting websites using <strong>HTML, CSS, and JavaScript</strong>—exploring frameworks like <strong>React</strong>, <strong>Node.js</strong>, and <strong>Express.js</strong>. As I progressed, I expanded my skills into backend development with <strong>MongoDB</strong>, <strong>MySQL</strong>, and even <strong>C++</strong>.\n</p>\n\n<p align=\"sd\" style=\"max-width: 800px; margin: auto; font-size: 17px; line-height: 1.6; color: #333;\">\n  With a strong foundation in <strong>Computer Science</strong>, I’ve deepened my knowledge through hands-on experience and continuous learning. \n  Over time, I’ve built and deployed impactful, real-world projects—from intelligent <strong>fraud detection systems</strong> and <strong>credit scoring models</strong> to robust <strong>medical data warehouses</strong> powered by <strong>FastAPI</strong> and <strong>YOLO</strong>.\n</p>\n\n<p align=\"gh\" style=\"max-width: 800px; margin: auto; font-size: 17px; line-height: 1.6; color: #333;\">\n  Today, I bring together the power of <strong>machine learning</strong>, <strong>web development</strong>, and <strong>data engineering</strong> to build scalable, human-centered solutions that make a difference.\n</p>\n\n\n<img align=\"left\" alt=\"coding\" width=\"400\" src=\"https://github.com/Adam-pw/Adam-pw/blob/main/animation_500_kxa883sd.gif?raw=true\">\n\n<p align=\"left\">\n  <img src=\"https://komarev.com/ghpvc/?username=dagiteferi&label=Profile%20views&color=0e75b6&style=flat\" alt=\"dagiteferi\" />\n</p>\n\n<br>\n<picture>\n  <img src=\"https://media.giphy.com/media/xUA7bdpLxQhsSQdyog/giphy.gif\" width=\"50px\" alt=\"About Me GIF\">\n</picture><strong>About Me</strong>\n<!-- <h3><strong>About Me</strong></h3> -->\n\n<ul>\n\n<li>🌱 Currently working on the exciting world of <strong>Python</strong>, <strong>AI</strong>, <strong>Data Science</strong>, and <strong>Machine Learning</strong>.</li>\n\n<li>🚀 Check out all of my projects here: <a href=\"https://dagmawipro.netlify.app/\" target=\"_blank\"><strong>dagmawipro.netlify.app</strong></a></li>\n<li>💬 Always happy to connect and chat about <strong>full-stack development</strong> or <strong>machine learning</strong> solutions.</li>\n <li>📫 You can reach me at: <a href=\"mailto:dagiteferi2011@gmail.com\">dagiteferi2011@gmail.com</a></li>\n</ul>\n \n</br>\n\n<h2 align=\"left\">connected with me </h2>\n<div align=\"center\">\n<a href=\"https://linkedin.com/in/https://www.linkedin.com/in/dagmawi-teferi\" target=\"blank\"><img align=\"center\" src=\"https://raw.githubusercontent.com/rahuldkjain/github-profile-readme-generator/master/src/images/icons/Social/linked-in-alt.svg\" alt=\"https://www.linkedin.com/in/dagmawi-teferi\" height=\"30\" width=\"40\" /></a>\n<a href=\"https://instagram.com/https://www.instagram.com/dagmawi_teferi/\" target=\"blank\"><img align=\"center\" src=\"https://raw.githubusercontent.com/rahuldkjain/github-profile-readme-generator/master/src/images/icons/Social/instagram.svg\" alt=\"https://www.instagram.com/dagmawi_teferi/\" height=\"30\" width=\"40\" /></a>  \n \n</div> \n\n<br>\n\n##  Technologies and Tools\n<p align=\"left\"> \n<a href=\"https://www.w3.org/html/\" target=\"_blank\"> <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/html5/html5-original-wordmark.svg\" alt=\"html5\" width=\"40\" height=\"40\"/> </a>\n    <a href=\"https://www.w3schools.com/css/\" target=\"_blank\"> <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/css3/css3-original-wordmark.svg\" alt=\"css3\" width=\"40\" height=\"40\"/> </a>\n <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript\" target=\"_blank\"> <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/javascript/javascript-original.svg\" alt=\"javascript\" width=\"40\" height=\"40\"/>\n <a href=\"https://www.w3schools.com/cpp/\" target=\"_blank\" rel=\"noreferrer\"> <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/cplusplus/cplusplus-original.svg\" alt=\"cplusplus\" width=\"40\" height=\"40\"/> </a><a href=\"https://www.djangoproject.com/\" target=\"_blank\" rel=\"noreferrer\"> <img src=\"https://cdn.worldvectorlogo.com/logos/django.svg\" alt=\"django\" width=\"40\" height=\"40\"/> <a href=\"https://www.postman.com/\" target=\"_blank\"> <img src=\"https://www.vectorlogo.zone/logos/getpostman/getpostman-icon.svg\" alt=\"postman\" width=\"40\" height=\"40\"/> </a>  \n  <a href=\"https://git-scm.com/\" target=\"_blank\"> <img src=\"https://www.vectorlogo.zone/logos/git-scm/git-scm-icon.svg\" alt=\"git\" width=\"40\" height=\"40\"/> </a>\n <a href=\"https://nodejs.org\" target=\"_blank\"> <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/nodejs/nodejs-original-wordmark.svg\" alt=\"nodejs\" width=\"40\" height=\"40\"/> </a>\n  <a href=\"https://reactjs.org/\" target=\"_blank\"> <img src=\"https://raw.githubusercontent.com/devicons/devicon/master/icons/react/react-original-wordmark.svg\" alt=\"react\" width=\"40\" height=\"40\"/> </a>\n\n \n![React](https://img.shields.io/badge/react-%2320232a.svg?style=for-the-badge&logo=react&logoColor=%2361DAFB)\n![MongoDB](https://img.shields.io/badge/MongoDB-%234ea94b.svg?style=for-the-badge&logo=mongodb&logoColor=white)\n![MySQL](https://img.shields.io/badge/mysql-%2300f.svg?style=for-the-badge&logo=mysql&logoColor=white)\n![Node.js](https://img.shields.io/badge/node.js-6DA55F?style=for-the-badge&logo=node.js&logoColor=white)\n![Express.js](https://img.shields.io/badge/express.js-%23404d59.svg?style=for-the-badge&logo=express&logoColor=%2361DAFB)\n![PostgreSQL](https://img.shields.io/badge/postgresql-%23316192.svg?style=for-the-badge&logo=postgresql&logoColor=white)\n![CSS3](https://img.shields.io/badge/css3-%231572B6.svg?style=for-the-badge&logo=css3&logoColor=white)\n![HTML5](https://img.shields.io/badge/html5-%23E34F26.svg?style=for-the-badge&logo=html5&logoColor=white)\n![JavaScript](https://img.shields.io/badge/javascript-%23F7DF1E.svg?style=for-the-badge&logo=javascript&logoColor=black)\n![NPM](https://img.shields.io/badge/npm-%23CB3837.svg?style=for-the-badge&logo=npm&logoColor=white)\n![Visual Studio Code](https://img.shields.io/badge/VS%20Code-0078d7?style=for-the-badge&logo=visual%20studio%20code&logoColor=white)\n![Sublime Text](https://img.shields.io/badge/sublime_text-%23575757.svg?style=for-the-badge&logo=sublime-text&logoColor=important)\n![SQL](https://img.shields.io/badge/sql-%230066CC.svg?style=for-the-badge&logo=sql&logoColor=white)\n![Yarn](https://img.shields.io/badge/yarn-%232C8EBB.svg?style=for-the-badge&logo=yarn&logoColor=white)\n![Python](https://img.shields.io/badge/python-%233776AB.svg?style=for-the-badge&logo=python&logoColor=white)\n![Jupyter Notebook](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=for-the-badge&logo=Jupyter&logoColor=white)\n![DVC](https://img.shields.io/badge/DVC-%2306090A.svg?style=for-the-badge&logo=dataversioncontrol&logoColor=white)\n![PostgreSQL](https://img.shields.io/badge/PostgreSQL-%23336791.svg?style=for-the-badge&logo=PostgreSQL&logoColor=white)\n![Docker](https://img.shields.io/badge/Docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white)\n![LangGraph](https://img.shields.io/badge/LangGraph-%2300C4B4.svg?style=for-the-badge&logo=langgraph&logoColor=white)\n![Cursor](https://img.shields.io/badge/Cursor-%236B46C1.svg?style=for-the-badge&logo=cursor&logoColor=white)\n\n\n<br><br>\n## 🏆 GitHub Trophies\n![](https://github-profile-trophy.vercel.app/?username=dagiteferi&theme=radical&no-frame=false&no-bg=true&margin-w=4)\n\n### ⚙️ &nbsp;GitHub Analytics\n<p>\n \n<p><img align=\"left\" src=\"https://github-readme-stats.vercel.app/api/top-langs?username=dagiteferi&show_icons=true&locale=en&layout=compact\" alt=\"dagiteferi\" /></p>\n\n<p>&nbsp;<img align=\"center\" src=\"https://github-readme-stats.vercel.app/api?username=dagiteferi&show_icons=true&locale=en\" alt=\"dagiteferi\" /></p>\n\n<p><img align=\"center\" src=\"https://github-readme-streak-stats.herokuapp.com/?user=dagiteferi&\" alt=\"dagiteferi\" /></p>\n</p>\n\n\n### 🔝 Top Contributed Repo\n![](https://github-contributor-stats.vercel.app/api?username=dagiteferi&limit=5&theme=dark&combine_all_yearly_contributions=true)"
    },
    {
        "title": "EthioMart-Amharic-NERLLM-Model",
        "url": "https://github.com/dagiteferi/EthioMart-Amharic-NERLLM-Model",
        "content": "This repository focuses on fine-tuning Named Entity Recognition (NER) models for the Amharic language, aimed at extracting key entities such as product names, prices, and locations from Ethiopian-based e-commerce Telegram channels.Dataset Source: Telegram e-commerce channels (e.g. @mertteka)\n\nREADME:\n# Amharic Named Entity Recognition (NER) System for EthioMart\r\n\r\n## Overview\r\nThis repository contains the implementation of a Named Entity Recognition (NER) system tailored for the Amharic language. The system is built using XLM-RoBERTa, a state-of-the-art multilingual transformer model, and is designed to extract key entities such as product names, prices, and locations from Amharic text data. The primary application of this system is for EthioMart, leveraging insights from Telegram messages.\r\n\r\n## Features\r\n- **Amharic Text Segmentation**: Utilizes the amseg library for accurate tokenization of Amharic text.\r\n- **Data Cleaning and Preprocessing**: Handles morphological complexities, removes unnecessary characters, and aligns tokens with labels.\r\n- **Fine-Tuned Transformer Model**: Employs XLM-RoBERTa for robust multilingual NER tasks.\r\n- **Performance Metrics**: Evaluates model performance using precision, recall, and F1-score.\r\n\r\n## Dataset\r\nThe dataset consists of Amharic text messages scraped from Telegram channels such as @MerttEka. These messages provide a rich source of information for identifying products, prices, and locations.\r\n\r\n## Installation\r\n1. **Clone the repository**:\r\n    ```sh\r\n    git clone https://github.com/your_username/dagiteferi-ethiomart-amharic-nerllm-model.git\r\n    cd dagiteferi-ethiomart-amharic-nerllm-model\r\n    ```\r\n2. **Create a virtual environment**:\r\n    ```sh\r\n    python -m venv venv\r\n    source venv/bin/activate  # On Windows: venv\\Scripts\\activate\r\n    ```\r\n3. **Install dependencies**:\r\n    ```sh\r\n    pip install -r requirements.txt\r\n    ```\r\n\r\n## Usage\r\n### Data Preparation\r\n- Run the `scrapper.py` script located in the `scripts/` directory to scrape data.\r\n- Use the provided preprocessing scripts to clean and label the data:\r\n    ```sh\r\n    python scripts/preprocessing.py\r\n    ```\r\n\r\n### Model Training\r\n- Fine-tune the model using the notebook:\r\n    ```sh\r\n    notebooks/XLM_Fine-tune.ipynb\r\n    ```\r\n\r\n### Evaluation\r\n- Evaluate the model’s performance:\r\n    ```sh\r\n    python scripts/evaluate_model.py\r\n    ```\r\n\r\n## Results\r\nThe fine-tuned model achieved the following performance metrics:\r\n- **Precision**: 99.9%\r\n- **Recall**:1.00\r\n- **F1-Score**: 1.00\r\n\r\n## Folder Structure\r\nDirectory structure:\r\n ```sh\r\n└── dagiteferi-ethiomart-amharic-nerllm-model/\r\n    ├── README.md\r\n    ├── requirements.txt\r\n    ├── scraping_session.session\r\n    ├── notebooks/\r\n    │   ├── README.md\r\n    │   ├── XLM_Fine-tune.ipynb\r\n    │   ├── __init__.py\r\n    │   ├── label.ipynb\r\n    │   ├── preprocessing.ipynb\r\n    │   └── token.ipynb\r\n    ├── scripts/\r\n    │   ├── README.md\r\n    │   ├── __init__.py\r\n    │   ├── labeling.py\r\n    │   ├── preprocessing.py\r\n    │   ├── scrapper.py\r\n    │   └── __pycache__/\r\n    ├── src/\r\n    │   ├── __init__.py\r\n    │   └── file_structure.py\r\n    ├── tests/\r\n    │   └── __init__.py\r\n    └── .github/\r\n        └── workflows/\r\n            └── unittests.yml\r\n```\r\n## Limitations and Future Work\r\n- **Incomplete Coverage**: Expand on tasks such as model comparison and interpretability.\r\n- **Dataset Diversity**: Incorporate more diverse sources to improve generalization.\r\n- **Documentation**: Enhance inline comments and examples for better clarity.\r\n\r\n\r\n\r\n## Acknowledgements\r\n- 10 Academy Team\r\n\r\n- Telegram Channels for Data Collection"
    },
    {
        "title": "Ethiopian-Medical-DataWarehouse",
        "url": "https://github.com/dagiteferi/Ethiopian-Medical-DataWarehouse",
        "content": "Building a data warehouse for Ethiopian medical businesses by scraping data from Telegram channels. The project integrates object detection using YOLO and provides CRUD operations via FastAPI to enable comprehensive data analysis and user interaction.\n\nREADME:\n# Telegram-Ethiopian-Medical-channel-data-warehouse\r\n\r\n## Table of Contents\r\n- [Overview](#overview)\r\n- [Technologies](#technologies)\r\n- [Folder Organization](#folder-organization)\r\n- [Setup](#setup)\r\n- [Notes](#notes)\r\n- [Contributing](#contributing)\r\n\r\n\r\n## Overview\r\n### Project Overview\r\nThis project aims to build a data warehouse to store data on Ethiopian medical businesses scraped from the web and Telegram channels. The data warehouse will enable comprehensive analysis, identification of trends, and efficient querying and reporting. Additionally, the project integrates object detection capabilities using YOLO (You Only Look Once) to enhance data analysis\r\n### Business Need\r\nA centralized data warehouse allows for efficient and effective data analysis. By consolidating fragmented data, we can uncover valuable insights about Ethiopian medical businesses, leading to informed decision-making.\r\n\r\n### Key Features\r\n### Data Scraping and Collection Pipeline\r\n\r\n- Extract data from relevant Telegram channels.\r\n- Collect images for object detection.\r\n- Store raw data in a temporary location..\r\n\r\n### Data Cleaning and Transformation\r\n\r\n- Remove duplicates, handle missing values, standardize formats, and validate data.\r\n- Store cleaned data in a database.\r\n- Transform data using DBT (Data Build Tool).\r\n\r\n### Object Detection Using YOLO\r\n\r\n- Set up the environment and download the YOLO model.\r\n- Detect objects in images.\r\n- Store detection data in a database table.\r\n\r\n### Data Exposure Using FastAPI\r\n\r\n- Set up a FastAPI application.\r\n- Configure database connection.\r\n- Define data models and schemas.\r\n- Implement CRUD operations.\r\n- Create API endpoints.\r\n\r\n## Technologies\r\n- Telethon\r\n- PostgreSQL\r\n- YOLO (You Only Look Once)\r\n- FastAPI\r\n- DBT(Data Build Tool)\r\n- Python\r\n\r\n## Folder Organization\r\n ```bash\r\n   Directory structure:\r\n└── Ethiopian-medical-datawarehouse\r\n    ├── requirements.txt\r\n    ├── yolov5s.pt\r\n    ├── API/\r\n    │   ├── crud.py\r\n    │   ├── database.py\r\n    │   ├── main.py\r\n    │   ├── models.py\r\n    │   ├── schemas.py\r\n    │   ├── frontEnd/\r\n    │   │   ├── index.html\r\n    │   │   ├── script.js\r\n    │   │   └── style.css\r\n    ├── data_house/\r\n    │   ├── dbt_project.yml\r\n    │   ├── .gitignore\r\n    │   ├── analyses/\r\n    │   │   └── .gitkeep\r\n    │   ├── logs/\r\n    │   ├── macros/\r\n    │   │   └── .gitkeep\r\n    │   ├── models/\r\n    │   │   ├── example/\r\n    │   │   │   ├── my_first_dbt_model.sql\r\n    │   │   │   ├── my_second_dbt_model.sql\r\n    │   │   │   ├── schema.yml\r\n    │   │   │   └── transform_messages.sql\r\n    │   │   └── sources/\r\n    │   │       └── sources.yml\r\n    │   ├── seeds/\r\n    │   │   └── .gitkeep\r\n    │   ├── snapshots/\r\n    │   │   └── .gitkeep\r\n    │   │   ├── compiled/\r\n    │   │   │   └── data_house/\r\n    │   │   │       └── models/\r\n    │   │   │           └── example/\r\n    │   │   │               ├── my_first_dbt_model.sql\r\n    │   │   │               ├── my_second_dbt_model.sql\r\n    │   │   │               ├── transform_messages.sql  \r\n    ├── notebooks/\r\n    │   ├── README.md\r\n    │   ├── DataCleaningTransformation.ipynb\r\n    │   ├── YOLO.ipynb\r\n    │   ├── __init__.py\r\n    │   └── detection_data_cleaning_load.ipynb\r\n    ├── scripts/\r\n    │   ├── README.md\r\n    │   ├── DataCleaningTransformation.py\r\n    │   ├── __init__.py\r\n    │   ├── database.py\r\n    │   ├── detection_data_cleaning.py\r\n    │   ├── load_detection_data.py\r\n    │   ├── yolo_object_detection.py\r\n    ├── src/\r\n    │   ├── __init__.py\r\n    │   ├── file_structure.py\r\n    │   └── scraper.py\r\n    ├── tests/\r\n    │   └── __init__.py\r\n    └── .github/\r\n        └── workflows/\r\n            └── unittests.yml\r\n\r\n```\r\n\r\n\r\n## Setup\r\n1. Clone the repository:\r\n   ```bash\r\n   git clone https://github.com/dagiteferi/Telegram-Ethiopian-Medical-channel-data-warehouse.git\r\n   cd Telegram-Ethiopian-Medical-channel-data-warehouse\r\n```\r\n2. Install the required dependencies:\r\n```bash\r\npip install -r requirements.txt\r\n```\r\n3. change directory to run the AIP locally.\r\n```bash\r\n   cd src\r\n```\r\n## Notes\r\n- Ensure you have all the necessary API keys and access tokens for Telegram.\r\n- Follow the instructions in the requirements.txt file to install all dependencies.\r\n\r\n## Contributing\r\nFork the repository.\r\nCreate your feature branch \r\n```bash\r\n(git checkout -b feature/AmazingFeature).\r\n```\r\nCommit your changes \r\n```bash\r\n(git commit -m 'Add some AmazingFeature')\r\n```\r\nPush to the branch \r\n```bash\r\n(git push origin feature/AmazingFeature).\r\n```\r\n\r\n\r\n\r\n\r\n\r\nOpen a Pull Request."
    },
    {
        "title": "fraud-detection-models",
        "url": "https://github.com/dagiteferi/fraud-detection-models",
        "content": "This project aims to develop the detection of fraudulent transactions in e-commerce using advanced machine learning techniques.It involves data analysis, preprocessing, feature engineering, model building, and deployment.The project includes API with Flask, containerization with Docker, and dashboards with Dash .\n\nREADME:\n# 🚀 Fraud Detection System  \n\nA machine learning-powered fraud detection system for e-commerce and banking transactions. This project includes data preprocessing, feature engineering, model training, explainability (SHAP & LIME), API development with Flask, deployment with Docker, and visualization with Dash.  \n\n## 📌 Project Overview  \nFraud detection is critical for securing online transactions and banking operations. This system detects fraudulent activities in e-commerce and bank credit transactions using advanced machine learning techniques and real-time monitoring.  \n\n## 📑 Table of Contents  \n- [Project Overview](#-project-overview)  \n- [Key Features](#-key-features)  \n- [Project Structure](#-project-structure)  \n- [Tech Stack](#-tech-stack)  \n- [Installation](#-installation)  \n- [Running with Docker](#-running-with-docker)  \n- [Dashboard Features](#-dashboard-features)  \n\n\n\n### 🔹 Key Features  \n✔️ Data preprocessing and feature engineering  \n✔️ Fraud detection model training with multiple algorithms  \n✔️ Model explainability using SHAP and LIME  \n✔️ REST API for real-time fraud detection (Flask)  \n✔️ Deployment using Docker  \n✔️ Interactive fraud analysis dashboard (Dash)  \n\n## 📂 Project Structure  \n ```bash\n   📂 dagiteferi-fraud-detection-models/\n├── 📜 README.md\n├── 📜 requirements.txt\n├── 🏠 fraud_detection_app/\n│   ├── 📦 Dockerfile\n│   ├── 🔄 callbacks.py\n│   ├── 📜 requirements.txt\n│   ├── 🚀 serve_model.py\n│   ├── 📝 .http\n│   ├── 📂 assets/\n│   │   ├── 🎨 styles.css\n│   │   ├── 📜 scripts.js\n├── 📂 logs/\n├── 📖 notebooks/\n│   ├── 📜 README.md\n│   ├── 📊 Data Analysis Preprocessing.ipynb\n│   ├── 🧐 Model_Explainability.ipynb\n│   ├── 🤖 model_Training_credit_card.ipynb\n│   ├── 🔍 model_Training_fraud_data.ipynb\n│   ├── 📜 __init__.py\n├── 📝 scripts/\n│   ├── 📜 README.md\n│   ├── ⚙️ FeatureEngineering.py\n│   ├── 🧐 Model_Explainability.py\n│   ├── 📊 bivariate.py\n│   ├── 📝 logger.py\n│   ├── 🤖 model.py\n│   ├── 📈 univariate.py\n│   ├── 📜 __init__.py\n├── 🔧 src/\n│   ├── 📜 __init__.py\n│   ├── 📂 data_loading.py\n│   ├── 📂 file_structure.py\n├── 🧪 tests/\n│   ├── 📜 __init__.py\n├── 🏗️ .github/\n│   ├── 📂 workflows/\n│   │   ├── 🔄 unittests.yml\n\n```\n\n## 🛠 Tech Stack  \n- **Programming Language:** Python (Pandas, NumPy, Scikit-learn, TensorFlow/PyTorch)  \n- **Machine Learning Models:** Logistic Regression, Random Forest, Gradient Boosting, LSTM, CNN  \n- **API & Deployment:** Flask, Docker  \n- **Explainability:** LIME  \n- **Visualization:** Dash, Matplotlib, Seaborn  \n\n## 🔧 Installation  \n\n### 1️⃣ Clone the repository  \n```sh\ngit clone https://github.com/dagiteferi/fraud-detection-models.git\ncd fraud-detection-models\n```\n2️⃣ Create a virtual environment\n```sh\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n```\n###  3️⃣ Install dependencies\n```sh\npip install -r requirements.txt\n```\n### 4️⃣ Run the API\n```sh\ncd fraud_detection_app\npython serve_model.py\n```\nThe API will run at http://127.0.0.1:5000/.\n### 5️⃣ Run the Dashboard\n```sh\ncd fraud_detection_app\npython serve_model.py\n```\nThe Dashboard will run at http://127.0.0.1:5000/.\n## 🚀 Running with Docker\n1️⃣ Build the Docker Image\n```sh\ndocker build -t fraud-detection-model -f fraud_detection_app/Dockerfile .\n\n```\n2️⃣ Run the Docker Container\n```sh\ndocker run -d -p 5000:5000 --name fraud-detection-container fraud-detection-model\n```\nThe API will be accessible at http://127.0.0.1:5000/ inside the container.\n\n## 📊 Dashboard Features\n## Dashboard Features\n\n1. **Fraud Detection Summary**:\n   - **Total Transactions**: Displays the total number of transactions in the dataset.\n   - **Fraud Cases**: Shows the total number of fraudulent transactions.\n   - **Fraud Percentage**: Displays the percentage of fraudulent transactions out of the total.\n\n2. **Fraud Trends Over Time**:\n   - A time series graph visualizing the number of fraud cases over time. It helps track how fraud patterns evolve.\n\n3. **Geographic Fraud Analysis**:\n   - A bar chart representing the fraud cases grouped by geographic locations (IP addresses). This helps identify regions with high fraudulent activity.\n\n4. **Device-based Fraud Analysis**:\n   - A bar chart showing fraud cases broken down by device ID, allowing identification of devices commonly used for fraudulent transactions.\n\n5. **Browser-based Fraud Analysis**:\n   - A bar chart comparing fraud cases across different browsers, helping to identify any browser-specific fraud patterns."
    },
    {
        "title": "insurance-risk-analytics",
        "url": "https://github.com/dagiteferi/insurance-risk-analytics",
        "content": "Insurance Risk Analytics Comprehensive data analytics project to optimize car insurance strategies for AlphaCare Insurance Solutions. Includes EDA, statistical modeling, and A/B testing to identify low-risk clients and enhance marketing effectiveness using historical insurance claim data.\n\nREADME:\n# Insurance Risk Analytics\r\n\r\n## Table of Contents \r\n1.[Project Overview](#project-overview)\r\n2.[Business Objective](#business-objective)\r\n3.[Data](#data)\r\n4.[Tasks](#Tasks)\r\n5.[Installation](#installation)\r\n6.[Usage](#usage)\r\n7.[Directory Structure](#directory-structure)\r\n8.[Contributing](#contributing)\r\n\r\n## Project Overview \r\nWelcome to the Insurance Risk Analytics project. This initiative is aimed at optimizing car insurance strategies for AlphaCare Insurance Solutions. By leveraging advanced data analytics and machine learning techniques, this project focuses on analyzing historical insurance claim data to identify low-risk clients, optimize marketing strategies, and ultimately improve business effectiveness.\r\n\r\n## Business Objective \r\nAlphaCare Insurance Solutions aims to develop cutting-edge risk and predictive analytics to optimize car insurance planning and marketing in South Africa. This project will:\r\n- Analyze historical insurance claim data\r\n- Help optimize marketing strategies\r\n- Discover low-risk clients for potential premium reductions\r\n\r\n  ## Data\r\nThe historical insurance claim data spans from February 2014 to August 2015. It includes various features related to insurance policies, clients, locations, vehicles, plans, and payments.\r\n\r\n## Tasks\r\n### Task 1: Git and GitHub\r\n- **Create a Git Repository**.\r\n- **Git Version Control**: Use Git for version control, commit changes regularly.\r\n- -**CI/CD with GitHub Actions**: Implement CI/CD pipelines.\r\n-**Project Planning - EDA & Stats**:\r\n- **Data Understanding**\r\n- **Exploratory Data Analysis (EDA)**\r\n- **Statistical Thinking**\r\n\r\n### Task 2: Data Version Control (DVC) \r\n- **Install and Initialize DVC**: Set up DVC to manage datasets.\r\n**Set Up Local Remote Storage**: Create storage for data tracking.\r\n- **Add Data and Commit Changes**: Track datasets with DVC and commit changes to version control.\r\n- **Push Data to Local Remote**: Ensure data is pushed to remote storage.\r\n\r\n  ### Task 3: A/B Hypothesis Testing\r\n- **Accept or Reject Null Hypotheses**:\r\n- No risk differences across provinces.\r\n- No risk differences between zip codes.\r\n- No significant margin difference between zip codes.\r\n- No significant risk difference between Women and Men.\r\n\r\n### Task 4: Statistical Modeling \r\n- **Data Preparation**: Handle missing data, perform feature engineering, encode categorical data, and split data into train/test sets.\r\n-**Modeling Techniques**: Implement models like Linear Regression, Decision Trees, Random Forests, and XGBoost.\r\n- **Model Evaluation**: Evaluate models using metrics like accuracy, precision, recall, and F1-score.\r\n- **Feature Importance Analysis**: Use SHAP or LIME to interpret model\r\n\r\n  ## Installation\r\nTo get started, clone the repository and install the required dependencies.\r\n\r\n```bash \r\ngit clone https://github.com/dagiteferi/insurance-risk-analytics.git\r\n```\r\n```bash \r\ncd insurance-risk-analytics\r\n```\r\n```bash \r\npython -m venv venv source venv/bin/activate\r\n```\r\n# On Windows use\r\n```bash \r\n`.\\venv\\Scripts\\activate`\r\n```\r\n```bash \r\n pip install -r requirements.txt\r\n```\r\n\r\n\r\n### Usage\r\nRun the analysis scripts to perform data analysis and model training. Detailed instructions for each task can be found in the respective directories.\r\n\r\n### Directory Structure\r\n```\r\ninsurance-risk-analytics/\r\n├── .vscode/\r\n│   └── settings.json\r\n├── .github/\r\n│   └── workflows/\r\n│       └── unittests.yml\r\n├── data/\r\n│   \r\n├── notebooks/\r\n│   ├── __init__.py\r\n│   └── README.md\r\n├── scripts/\r\n│   ├── __init__.py\r\n│   └── README.md\r\n├── src/\r\n\r\n│   ├── __init__.py\r\n├── tests/\r\n│   └── __init__.py\r\n├── .gitignore\r\n├── requirements.txt\r\n├── README.md\r\n```\r\n### Contributing\r\nContributions are welcome! Please fork the repository and create a pull request to propose changes."
    },
    {
        "title": "library-managment-sysem",
        "url": "https://github.com/dagiteferi/library-managment-sysem",
        "content": "the c++ program that store , update,delete data ,sort book by name ,ISBN number, it also store the data in database.txt file \n\nREADME:\n# library-managment-sysem\n the c++ program that store , update,delete data ,sort book by name ,ISBN number, it also store the data in database.txt file"
    },
    {
        "title": "sales-Prediction-model",
        "url": "https://github.com/dagiteferi/sales-Prediction-model",
        "content": "An end-to-end machine learning solution for forecasting sales across all stores of Rossmann Pharmaceuticals in several cities, six weeks ahead. Accurate sales predictions will assist the finance team in better planning and decision-making\n\nREADME:\n# sales-Prediction-model\n\n## Table of Contents\n\n1. [Project Objectives](#project-objectives)\n   1. [Data Cleaning and Preparation](#data-cleaning-and-preparation)\n   2. [Exploratory Data Analysis (EDA)](#exploratory-data-analysis-eda)\n   3. [Sales Prediction](#sales-prediction)\n   4. [Deep Learning](#deep-learning)\n   5. [Model Deployment](#model-deployment)\n2. [Data and Features](#data-and-features)\n3. [Tasks Breakdown](#tasks-breakdown)\n   1. [Task 1 - Exploration of Customer Purchasing Behavior](#task-1---exploration-of-customer-purchasing-behavior)\n   2. [Task 2 - Prediction of Store Sales](#task-2---prediction-of-store-sales)\n      1. [Preprocessing](#preprocessing)\n      2. [Building Models with sklearn Pipelines](#building-models-with-sklearn-pipelines)\n      3. [Choosing a Loss Function](#choosing-a-loss-function)\n      4. [Post Prediction Analysis](#post-prediction-analysis)\n      5. [Serialize Models](#serialize-models)\n      6. [Deep Learning Model](#deep-learning-model)\n   3. [Task 3 - Model Serving API Call](#task-3---model-serving-api-call)\n\n### Overview\n\nThis project aims to develop an end-to-end machine learning solution for forecasting sales across all stores of Rossmann Pharmaceuticals in several cities, six weeks ahead. Accurate sales predictions will assist the finance team in better planning and decision-making.\n\n## Project Objectives\n\n### Data Cleaning and Preparation:\n\nHandle outliers, missing data, and preprocess the dataset for analysis.\n\n### Exploratory Data Analysis (EDA):\n\nAnalyze and visualize data to understand customer purchasing behavior and the impact of various factors on sales.\n\n### Sales Prediction:\n\nBuild and fine-tune machine learning models using sklearn pipelines and tree-based algorithms to forecast daily sales.\n\n### Deep Learning:\n\nImplement a Long Short-Term Memory (LSTM) model to improve prediction accuracy.\n\n### Model Deployment:\n\nCreate a REST API to serve the trained models for real-time predictions using frameworks like Flask, FastAPI, or Django REST framework.\n\n## Data and Features\n\nThe dataset includes fields such as store IDs, sales, customers, indicators for store openings, holidays, promotions, assortment levels, competition details, and more. Key features for predicting sales include promotions, holidays, seasonality, competition, locality, and customer numbers.\n\n## Tasks Breakdown\n\n### Task 1 - Exploration of Customer Purchasing Behavior\n\n. Data cleaning and preparation.\n\n. Exploratory Data Analysis (EDA) with visualizations.\n\n. Addressing key questions about promotions, holiday sales behavior, seasonal patterns, and more.\n\n### Task 2 - Prediction of Store Sales\n\n#### Preprocessing:\n\nConvert non-numeric columns, handle NaN values, and generate new features from datetime columns.\n\n#### Building Models with sklearn Pipelines:\n\nStart with tree-based algorithms like Random Forests.\n\n#### Choosing a Loss Function:\n\nSelect and justify the appropriate loss function.\n\n#### Post Prediction Analysis:\nExplore feature importance and estimate confidence intervals.\n\n#### Serialize Models:\nSave models with timestamps for tracking.\n\n#### Deep Learning Model:\n\nImplement an LSTM model using TensorFlow or PyTorch.\n\n### Task 3 - Model Serving API Call\n\nCreate a REST API for real-time predictions.\n\nLoad the serialized model and define API endpoints.\n\nPreprocess input data, make predictions, and format the results.\n\nDeploy the API to a web server or cloud platform.\n\n## Setup\n\nTo get started with this project, follow these steps:\n\n1. **Clone the repository:**\n\n   ```sh\n   git clone https://github.com/dagiteferi/sales-Prediction-model.git\n   cd sales-Prediction-model\n   ```\n\n2. **Create and activate a virtual environment:**\n\n   ```sh\n   python -m venv env\n   source env/bin/activate  # On Windows use `env\\Scripts\\activate`\n   ```\n\n3. **Install the required packages:**\n\n   ```sh\n   pip install -r requirements.txt\n   ```\n\n4. **Run the Jupyter Notebook:**\n\n   ```sh\n   jupyter notebook\n   ```\n\n5. **Run the Flask application:**\n   ```sh\n   flask run\n   ```"
    },
    {
        "title": "Search-And-Sort-Algorithms-using-c-",
        "url": "https://github.com/dagiteferi/Search-And-Sort-Algorithms-using-c-",
        "content": "in this program you can find #search #algorithms(#Linear_search, #Binary_search in descend and ascend order)  and #Sort #algorithms(#buble,#Inserttion,#selection_sort)"
    },
    {
        "title": "solar-farm-data_analysis",
        "url": "https://github.com/dagiteferi/solar-farm-data_analysis",
        "content": "for MoonLight Energy Solutions focuses on crafting a strategic approach to enhance operational efficiency and sustainability through targeted solar investments. KAIM -W0"
    },
    {
        "title": "Spiritual-Tracker",
        "url": "https://github.com/dagiteferi/Spiritual-Tracker",
        "content": "Spiritual Tracker App for Ethiopian Gospel Believers Church Overview The Spiritual Tracker App supports the spiritual journey of the Ethiopian Gospel Believers Church community. It provides daily Bible verses and devotions, a platform for prayer requests, an event calendar, and access to sermon recordings.\n\nREADME:\n# Spiritual Tracker App for Ethiopian Gospel Believers Church\n\n## Overview\nThe **Spiritual Tracker App** is designed to support the spiritual journey of the Ethiopian Gospel Believers Church community. This app provides daily Bible verses and devotions, a platform for submitting and viewing prayer requests, an event calendar for upcoming church activities, and access to sermon recordings.\n\n## Features\n1. **Daily Bible Verses and Devotions**:\n   - Receive a daily Bible verse and a short devotional to inspire and guide you throughout the day.\n\n2. **Prayer Requests**:\n   - Submit your prayer requests and view communal prayers from fellow church members.\n\n3. **Church Event Calendar**:\n   - Stay updated with a simple calendar featuring upcoming church events and services.\n\n4. **Sermon Recordings**:\n   - Access and listen to recorded sermons anytime, anywhere.\n\n## Installation\n1. **Clone the repository**:\n```bash \ngit clone https://github.com/your-username/spiritual-tracker-app.git\ncd spiritual-tracker-app\n```\n## Install dependencies:\n```bash\nflutter pub get\n```\n## Run the app:\n```bash\nflutter run\n```\n## Usage\nDaily Devotions: Open the app to view the daily Bible verse and devotional.\n\nPrayer Requests: Navigate to the Prayer Requests section to submit a new request or view existing ones.\n\nEvent Calendar: Check the Event Calendar for upcoming church events and services.\n\nSermon Recordings: Listen to recorded sermons in the Sermon Recordings section.\n\n## Contributing\nWe welcome contributions from the community! To contribute:\n\nFork the repository.\n\nCreate a new branch\n\n```bash\ngit checkout -b feature-branch\n```\n\n## Make your changes and commit them:\n```bash\ngit commit -m \"Description of your changes\"\n```\n\n## Push to the branch:\n```bash\ngit push origin feature-branch\n```"
    },
    {
        "title": "stu-infomation",
        "url": "https://github.com/dagiteferi/stu-infomation",
        "content": "aquataion"
    },
    {
        "title": "TimeSeries-Portfolio-Optimization",
        "url": "https://github.com/dagiteferi/TimeSeries-Portfolio-Optimization",
        "content": "This project aims to predict stock prices (TSLA, BND, SPY) using ARIMA, SARIMA, and LSTM models. It optimizes portfolios to maximize returns, minimize risks, and provides actionable insights for data-driven investment decisions.\n\nREADME:\n# Time Series Forecasting and Portfolio Optimization\r\n\r\n![GitHub](https://img.shields.io/badge/Python-3.8%2B-blue)\r\n![GitHub](https://img.shields.io/badge/License-MIT-green)\r\n![GitHub](https://img.shields.io/badge/Status-Completed-brightgreen)\r\n\r\n## 📑 Table of Contents\r\n- [Overview](#-overview)\r\n- [Key Features](#-key-features)\r\n- [Repository Structure](#-repository-structure)\r\n- [Installation](#-installation)\r\n- [Usage](#-usage)\r\n- [Results](#-results)\r\n- [License](#-license)\r\n- [Acknowledgments](#-acknowledgments)\r\n- [Contact](#-contact)\r\n\r\n---\r\n\r\n## 📌 Overview\r\nThis project focuses on leveraging **time series forecasting** and **portfolio optimization** to enhance investment strategies for **Guide Me in Finance (GMF) Investments**. Using historical financial data for **Tesla (TSLA)**, **Vanguard Total Bond Market ETF (BND)**, and **S&P 500 ETF (SPY)**, the project aims to:\r\n- Predict future stock prices using advanced forecasting models like **ARIMA**, **SARIMA**, and **LSTM**.\r\n- Optimize portfolio allocation to maximize returns while minimizing risks.\r\n- Provide actionable insights for financial analysts to make data-driven investment decisions.\r\n\r\n---\r\n\r\n## 🚀 Key Features\r\n- **Data Preprocessing**: Clean and prepare historical financial data for analysis.\r\n- **Exploratory Data Analysis (EDA)**: Visualize trends, volatility, and key metrics like Value at Risk (VaR) and Sharpe Ratio.\r\n- **Time Series Forecasting**: Implement ARIMA, SARIMA, and LSTM models to predict future stock prices.\r\n- **Portfolio Optimization**: Optimize asset allocation using covariance matrices and risk-return analysis.\r\n- **Visualizations**: Generate interactive charts and graphs for forecasts and portfolio performance.\r\n\r\n---\r\n\r\n## 📂 Repository Structure\r\n```bash\r\n  Directory structure:\r\n└── dagiteferi-timeseries-portfolio-optimization/\r\n    ├── README.md\r\n    ├── requirements.txt\r\n    ├── logs/\r\n    ├── model/\r\n    │   └── optimized_lstm.keras\r\n    ├── notebooks/\r\n    │   ├── README.md\r\n    │   ├── Future_Forecasting.ipynb\r\n    │   ├── Preprocess_Explore_Data.ipynb\r\n    │   ├── __init__.py\r\n    │   ├── arima_forecasting.ipynb\r\n    │   ├── lstm.ipynb\r\n    │   ├── portfolio_optimization.ipynb\r\n    │   └── sarima_forecasting.ipynb\r\n    ├── scripts/\r\n    │   ├── README.md\r\n    │   ├── __init__.py\r\n    │   ├── arima.py\r\n    │   ├── data_cleaning.py\r\n    │   ├── eda.py\r\n    │   ├── future_forecast.py\r\n    │   ├── lstm.py\r\n    │   ├── portfolio_optimization.py\r\n    │   ├── sarima.py\r\n    │   ├── seasonal_decompose.py\r\n    │   ├── volatility_analysis.py\r\n    │   └── __pycache__/\r\n    ├── src/\r\n    │   ├── __init__.py\r\n    │   ├── data_loader.py\r\n    │   ├── fetch_data.py\r\n    │   ├── file_structure.py\r\n    │   └── __pycache__/\r\n    ├── tests/\r\n    │   └── __init__.py\r\n    └── .github/\r\n        └── workflows/\r\n            └── unittests.yml\r\n\r\n   ```\r\n---\r\n\r\n## 🛠️ Installation\r\n1. Clone the repository:\r\n   ```bash\r\n   git clone https://github.com/dagiteferi/TimeSeries-Portfolio-Optimization.git\r\n   ```\r\n2.Navigate to the project directory:\r\n\r\n```bash\r\n  cd TimeSeries-Portfolio-Optimization\r\n   ```\r\n3.Install the required dependencies:\r\n```bash\r\n pip install -r requirements.txt\r\n   ```\r\n🧑‍💻 Usage\r\n1. Data Preprocessing:\r\n\r\nRun notebooks/01_Data_Preprocessing.ipynb to clean and prepare the data.\r\n\r\n2. Exploratory Data Analysis:\r\n\r\nUse notebooks/02_EDA.ipynb to visualize trends and analyze volatility.\r\n\r\n3. Time Series Forecasting:\r\n\r\nTrain and evaluate models in notebooks/03_TimeSeries_Forecasting.ipynb.\r\n\r\n4. Portfolio Optimization:\r\n\r\nOptimize asset allocation in notebooks/04_Portfolio_Optimization.ipynb.\r\n\r\n📊 Results\r\nForecasted stock prices with confidence intervals.\r\n\r\nOptimized portfolio allocations for maximum Sharpe Ratio.\r\n\r\nVisualizations of portfolio performance and risk-return analysis.\r\n\r\n🙏 Acknowledgments\r\nData sourced from YFinance.\r\n\r\nInspired by portfolio optimization techniques from PyPortfolioOpt.\r\n\r\n📧 Contact\r\nFor questions or feedback, feel free to reach out:\r\n📩 dagiteferi2011@gmail.com\r\n🌐(https://dagmawipro.netlify.app/)"
    },
    {
        "title": "User-Overview-Engagement-and-Experience-Analysis",
        "url": "https://github.com/dagiteferi/User-Overview-Engagement-and-Experience-Analysis",
        "content": "Comprehensive analysis of TellCo, focusing on user behavior, engagement, experience, and satisfaction. Includes data preparation, exploratory analysis, dashboard development, and predictive modeling to provide actionable insights for growth opportunities.\n\nREADME:\n# User Overview, Engagement, and Experience Analysis\r\n\r\nWelcome to the **User Overview, Engagement, and Experience Analysis** project! This repository contains a comprehensive analysis of TellCo, a mobile service provider, aimed at identifying growth opportunities and providing actionable insights for investors.\r\n\r\n## Table of Contents\r\n- [About](#about)\r\n- [Project Objectives](#project-objectives)\r\n- [Data Description](#data-description)\r\n- [Tasks](#tasks)\r\n  - [Task 1: User Overview Analysis](#task-1-user-overview-analysis)\r\n  - [Task 2: User Engagement Analysis](#task-2-user-engagement-analysis)\r\n  - [Task 3: Experience Analytics](#task-3-experience-analytics)\r\n  - [Task 4: Satisfaction Analysis](#task-4-satisfaction-analysis)\r\n  - [Task 5: Dashboard Development](#task-5-dashboard-development)\r\n- [Project Structure](#project-structure)\r\n- [Setup Instructions](#setup-instructions)\r\n- [Learning Outcomes](#learning-outcomes)\r\n- [Competencies](#competencies)\r\n- [Deliverables](#deliverables)\r\n\r\n## About\r\nThis project aims to analyze customer data from TellCo to uncover insights and provide data-driven recommendations on whether to purchase the company. The analysis includes user behavior, engagement, experience, and satisfaction, presented through a comprehensive dashboard and written reports.\r\n\r\n## Project Objectives\r\n- **User Overview Analysis**: Understand customer behavior and device preferences.\r\n- **User Engagement Analysis**: Assess user activity and engagement across various applications.\r\n- **Experience Analytics**: Evaluate customer experience based on network parameters and device characteristics.\r\n- **Satisfaction Analysis**: Combine engagement and experience metrics to analyze overall customer satisfaction.\r\n\r\n## Data Description\r\nThe data for this project comes from a month's aggregation of xDR records from TellCo's systems, extracted from a PostgreSQL database. It includes various attributes describing customer activities on the network.\r\n\r\n## Tasks\r\n\r\n### Task 1: User Overview Analysis\r\n- Identify top handsets and manufacturers.\r\n- Provide recommendations for marketing teams.\r\n- Aggregate user behavior data for applications.\r\n- Conduct exploratory data analysis (EDA).\r\n\r\n### Task 2: User Engagement Analysis\r\n- Assess user engagement using session frequency, duration, and traffic metrics.\r\n- Normalize metrics and classify customers into engagement groups.\r\n- Visualize and interpret engagement metrics.\r\n\r\n### Task 3: Experience Analytics\r\n- Evaluate customer experience based on network parameters and device characteristics.\r\n- Aggregate and analyze average TCP retransmission, RTT, and throughput.\r\n- Perform clustering to segment users based on experience.\r\n\r\n### Task 4: Satisfaction Analysis\r\n- Assign engagement and experience scores to users.\r\n- Calculate and report satisfaction scores.\r\n- Build a regression model to predict satisfaction.\r\n- Perform clustering on engagement and experience scores.\r\n\r\n### Task 5: Dashboard Development\r\n- Design and develop an interactive dashboard to visualize data insights.\r\n- Ensure usability, interactivity, and visual appeal.\r\n- Deploy the dashboard to a public URL.\r\n\r\n\r\n## Setup Instructions\r\n\r\n1. **🚀 Clone the repository**:\r\n   ```bash\r\n   git clone https://github.com/yourusername/your-repo-name.git\r\n   cd your-repo-name\r\n ```\r\n\r\n## Setup Instructions\r\n\r\n1. **🚀 Clone the repository**:\r\n   ```bash\r\n   git clone https://github.com/yourusername/your-repo-name.git\r\n   cd your-repo-name\r\n\r\n  ```\r\n## Create a virtual environment:\r\npython -m venv week-2\r\nsource week-2/bin/activate  # On Windows use `week-2\\Scripts\\activate`\r\n\r\n  \r\n##Install dependencies:\r\n ```bash\r\npip install -r requirements.txt\r\n\r\n ```"
    }
]